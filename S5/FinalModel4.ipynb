{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FinalModel4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "MiggylgS7xka"
      },
      "source": [
        "from __future__ import print_function\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "import torch.optim as optim\r\n",
        "from torchvision import datasets, transforms"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NERLnTNjOLQL"
      },
      "source": [
        "# Train Phase transformations\r\n",
        "train_transforms = transforms.Compose([\r\n",
        "                                      #  transforms.Resize((28, 28)),\r\n",
        "                                      #  transforms.ColorJitter(brightness=0.10, contrast=0.1, saturation=0.10, hue=0.1),\r\n",
        "                                       transforms.ToTensor(),\r\n",
        "                                       transforms.Normalize((0.1307,), (0.3081,)) # The mean and std have to be sequences (e.g., tuples), therefore you should add a comma after the values. \r\n",
        "                                       # Note the difference between (0.1307) and (0.1307,)\r\n",
        "                                       ])\r\n",
        "\r\n",
        "# Test Phase transformations\r\n",
        "test_transforms = transforms.Compose([\r\n",
        "                                      #  transforms.Resize((28, 28)),\r\n",
        "                                      #  transforms.ColorJitter(brightness=0.10, contrast=0.1, saturation=0.10, hue=0.1),\r\n",
        "                                       transforms.ToTensor(),\r\n",
        "                                       transforms.Normalize((0.1307,), (0.3081,))\r\n",
        "                                       ])\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3NfTt0uPHTB"
      },
      "source": [
        "train = datasets.MNIST('./data', train=True, download=True, transform=train_transforms)\r\n",
        "test = datasets.MNIST('./data', train=False, download=True, transform=test_transforms)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "8ORChCB9PPBK",
        "outputId": "83733ab3-61af-4620-e17d-fd67ee1f6d0c"
      },
      "source": [
        "image, label = next(iter(train))\r\n",
        "\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "plt.imshow(image.numpy().reshape(28,28),cmap = 'gray')\r\n",
        "\r\n",
        "print('LABEL : ' , label)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LABEL :  5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qe5waWYgOLSS",
        "outputId": "49e0ead6-94dd-4675-e0da-96d1cba7b81c"
      },
      "source": [
        "SEED = 1\r\n",
        "\r\n",
        "# CUDA?\r\n",
        "cuda = torch.cuda.is_available()\r\n",
        "print(\"CUDA Available?\", cuda)\r\n",
        "\r\n",
        "# For reproducibility\r\n",
        "torch.manual_seed(SEED)\r\n",
        "\r\n",
        "if cuda:\r\n",
        "    torch.cuda.manual_seed(SEED)\r\n",
        "\r\n",
        "# dataloader arguments - something you'll fetch these from cmdprmt\r\n",
        "dataloader_args = dict(shuffle=True, batch_size=128, num_workers=4, pin_memory=True) if cuda else dict(shuffle=True, batch_size=64)\r\n",
        "\r\n",
        "# train dataloader\r\n",
        "train_loader = torch.utils.data.DataLoader(train, **dataloader_args)\r\n",
        "\r\n",
        "# test dataloader\r\n",
        "test_loader = torch.utils.data.DataLoader(test, **dataloader_args)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CUDA Available? True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nE8SkOqXOLW6"
      },
      "source": [
        "class Net(nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super(Net, self).__init__()\r\n",
        "\r\n",
        "        dropout_rate = 0.01\r\n",
        "\r\n",
        "        self.conv1 = nn.Sequential(\r\n",
        "            nn.Conv2d(in_channels=1, out_channels=8, kernel_size=3),\r\n",
        "            nn.ReLU(),\r\n",
        "            nn.BatchNorm2d(8),\r\n",
        "            nn.Dropout(dropout_rate)\r\n",
        "        )  # Input: 28x28x1 | Output: 26x26x8 | RF: 3x3\r\n",
        "\r\n",
        "        self.conv2 = nn.Sequential(\r\n",
        "            nn.Conv2d(in_channels=8, out_channels=8, kernel_size=3),\r\n",
        "            nn.ReLU(),\r\n",
        "            nn.BatchNorm2d(8),\r\n",
        "            nn.Dropout(dropout_rate)\r\n",
        "        )  # Input: 26x26x8 | Output: 24x24x8 | RF: 5x5\r\n",
        "\r\n",
        "        self.conv3 = nn.Sequential(\r\n",
        "            nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3),\r\n",
        "            nn.ReLU(),\r\n",
        "            nn.BatchNorm2d(16),\r\n",
        "            nn.Dropout(dropout_rate)\r\n",
        "        )  # Input: 24x24x8 | Output: 22x22x16 | RF: 7x7\r\n",
        "\r\n",
        "        self.conv4 = nn.Sequential(\r\n",
        "            nn.Conv2d(in_channels=16, out_channels=16, kernel_size=3),\r\n",
        "            nn.ReLU(),\r\n",
        "            nn.BatchNorm2d(16),\r\n",
        "            nn.Dropout(dropout_rate)\r\n",
        "        )  # Input: 22x22x16 | Output: 20x20x16 | RF: 9x9\r\n",
        "\r\n",
        "        self.pool = nn.MaxPool2d(2, 2)  # Input: 20x20x16 | Output: 10x10x16 | RF: 10x10\r\n",
        "\r\n",
        "        self.conv5 = nn.Sequential(\r\n",
        "            nn.Conv2d(in_channels=16, out_channels=16, kernel_size=3),\r\n",
        "            nn.ReLU(),\r\n",
        "            nn.BatchNorm2d(16),\r\n",
        "            nn.Dropout(dropout_rate)\r\n",
        "        )  # Input: 10x10x16 | Output: 8x8x16 | RF: 14x14\r\n",
        "\r\n",
        "        self.conv6 = nn.Sequential(\r\n",
        "            nn.Conv2d(in_channels=16, out_channels=16, kernel_size=3),\r\n",
        "            nn.ReLU(),\r\n",
        "            nn.BatchNorm2d(16),\r\n",
        "            nn.Dropout(dropout_rate)\r\n",
        "        )  # Input: 8x8x16 | Output: 6x6x16 | RF: 18x18\r\n",
        "\r\n",
        "        self.conv7 = nn.Sequential(\r\n",
        "            nn.Conv2d(in_channels=16, out_channels=10, kernel_size=1),\r\n",
        "            nn.ReLU(),\r\n",
        "            nn.BatchNorm2d(10),\r\n",
        "            nn.Dropout(dropout_rate)\r\n",
        "        )  # Input: 6x6x16 | Output: 6x6x10 | RF: 18x18\r\n",
        "\r\n",
        "        self.gap = nn.Sequential(\r\n",
        "            nn.AdaptiveAvgPool2d(1)\r\n",
        "        )  # Input: 6x6x10 | Output: 1x1x10 | RF: 28x28\r\n",
        "    \r\n",
        "    def forward(self, x):\r\n",
        "\r\n",
        "        x = self.conv1(x)\r\n",
        "        x = self.conv2(x)\r\n",
        "        x = self.conv3(x)\r\n",
        "        x = self.conv4(x)\r\n",
        "\r\n",
        "        x = self.pool(x)\r\n",
        "\r\n",
        "        x = self.conv5(x)\r\n",
        "        x = self.conv6(x)\r\n",
        "        x = self.conv7(x)\r\n",
        "\r\n",
        "        x = self.gap(x)\r\n",
        "\r\n",
        "        x = x.view(-1, 10)\r\n",
        "\r\n",
        "        return F.log_softmax(x, dim=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQgdERLMC8Az"
      },
      "source": [
        "# !pip install torchsummary\r\n",
        "from torchsummary import summary\r\n",
        "use_cuda = torch.cuda.is_available()\r\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQ5dzh88bxP7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d79907b-1467-4bb2-e5c4-956d4680ce53"
      },
      "source": [
        "model = Net()\r\n",
        "model = Net().to(device)\r\n",
        "\r\n",
        "summary(model, input_size=(1, 28, 28))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1            [-1, 8, 26, 26]              80\n",
            "              ReLU-2            [-1, 8, 26, 26]               0\n",
            "       BatchNorm2d-3            [-1, 8, 26, 26]              16\n",
            "           Dropout-4            [-1, 8, 26, 26]               0\n",
            "            Conv2d-5            [-1, 8, 24, 24]             584\n",
            "              ReLU-6            [-1, 8, 24, 24]               0\n",
            "       BatchNorm2d-7            [-1, 8, 24, 24]              16\n",
            "           Dropout-8            [-1, 8, 24, 24]               0\n",
            "            Conv2d-9           [-1, 16, 22, 22]           1,168\n",
            "             ReLU-10           [-1, 16, 22, 22]               0\n",
            "      BatchNorm2d-11           [-1, 16, 22, 22]              32\n",
            "          Dropout-12           [-1, 16, 22, 22]               0\n",
            "           Conv2d-13           [-1, 16, 20, 20]           2,320\n",
            "             ReLU-14           [-1, 16, 20, 20]               0\n",
            "      BatchNorm2d-15           [-1, 16, 20, 20]              32\n",
            "          Dropout-16           [-1, 16, 20, 20]               0\n",
            "        MaxPool2d-17           [-1, 16, 10, 10]               0\n",
            "           Conv2d-18             [-1, 16, 8, 8]           2,320\n",
            "             ReLU-19             [-1, 16, 8, 8]               0\n",
            "      BatchNorm2d-20             [-1, 16, 8, 8]              32\n",
            "          Dropout-21             [-1, 16, 8, 8]               0\n",
            "           Conv2d-22             [-1, 16, 6, 6]           2,320\n",
            "             ReLU-23             [-1, 16, 6, 6]               0\n",
            "      BatchNorm2d-24             [-1, 16, 6, 6]              32\n",
            "          Dropout-25             [-1, 16, 6, 6]               0\n",
            "           Conv2d-26             [-1, 10, 6, 6]             170\n",
            "             ReLU-27             [-1, 10, 6, 6]               0\n",
            "      BatchNorm2d-28             [-1, 10, 6, 6]              20\n",
            "          Dropout-29             [-1, 10, 6, 6]               0\n",
            "AdaptiveAvgPool2d-30             [-1, 10, 1, 1]               0\n",
            "================================================================\n",
            "Total params: 9,142\n",
            "Trainable params: 9,142\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.81\n",
            "Params size (MB): 0.03\n",
            "Estimated Total Size (MB): 0.85\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "obUexR42DGmh"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\r\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr = 0.001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6iIeK3H6bjD5",
        "outputId": "cc3d7be1-db79-4d1a-b0ff-8ae4bf8e8944"
      },
      "source": [
        "epochs = 15\r\n",
        "\r\n",
        "train_losses = []\r\n",
        "test_losses = []\r\n",
        "\r\n",
        "train_correct = []\r\n",
        "test_correct = []\r\n",
        "\r\n",
        "for i in range(epochs):\r\n",
        "    \r\n",
        "    correct_classified = 0\r\n",
        "    for batch_number , (x_train,y_train) in enumerate(train_loader):\r\n",
        "        \r\n",
        "        batch_number+=1\r\n",
        "        \r\n",
        "        x_train,y_train = x_train.to(device), y_train.to(device)\r\n",
        "\r\n",
        "        # print(x_train.shape)\r\n",
        "        pred = model.forward(x_train)\r\n",
        "\r\n",
        "        # print(pred.shape)\r\n",
        "        # print(y_train.shape)\r\n",
        "        loss = criterion(pred,y_train)\r\n",
        "        \r\n",
        "        #pred.argmax(dim=1, keepdim=True)\r\n",
        "        #PyTorch .eq() function to do this, which compares the values in two tensors and if they match, returns a 1. If they don’t match, it returns a 0:\r\n",
        "        #correct += pred.eq(target.view_as(pred)).sum().item()\r\n",
        "        predicted = torch.max(pred.data ,1)[1]\r\n",
        "        correct_classified += (predicted == y_train).sum()\r\n",
        "        \r\n",
        "        optimizer.zero_grad()\r\n",
        "        loss.backward()\r\n",
        "        optimizer.step()\r\n",
        "        \r\n",
        "        if batch_number%100 == 0:\r\n",
        "            \r\n",
        "            acc = round((correct_classified.item())/(batch_number*128),5)\r\n",
        "            print(f'(TRAIN) Epoch: {i:4} batch_number: {batch_number:4} Loss : {loss:4.4} Acc : {acc:4.5}')\r\n",
        "        \r\n",
        "    train_losses.append(loss) \r\n",
        "    train_correct.append(correct_classified)\r\n",
        "    \r\n",
        "    \r\n",
        "    with torch.no_grad():\r\n",
        "        \r\n",
        "        test_loss = []\r\n",
        "        correct_classified = 0\r\n",
        "        for batch_number , (x_test,y_test) in enumerate(test_loader):\r\n",
        "        \r\n",
        "            x_test,y_test = x_test.to(device), y_test.to(device)\r\n",
        "            pred = model.forward(x_test)\r\n",
        "            loss = criterion(pred,y_test)\r\n",
        "            test_losses.append(loss)\r\n",
        "            \r\n",
        "            correct_classified += (torch.max(pred,1)[1] == y_test).sum()\r\n",
        "        \r\n",
        "        avg_loss = torch.mean(torch.tensor(test_losses))\r\n",
        "        test_losses.append(avg_loss)\r\n",
        "        test_correct.append(correct_classified)\r\n",
        "        \r\n",
        "        acc = round(correct_classified.item()/10000,5)\r\n",
        "        print('(TEST) Correct_classified : ' , correct_classified.item() ,' of 10000')\r\n",
        "        print(f'(TEST) Epoch: {i:4} Loss : {avg_loss:4.4} Acc : {acc:4.5}')\r\n",
        "        print('\\n','*'*60 , '\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(TRAIN) Epoch:    0 batch_number:  100 Loss : 0.8778 Acc : 0.75617\n",
            "(TRAIN) Epoch:    0 batch_number:  200 Loss : 0.5949 Acc : 0.84273\n",
            "(TRAIN) Epoch:    0 batch_number:  300 Loss : 0.4565 Acc : 0.87919\n",
            "(TRAIN) Epoch:    0 batch_number:  400 Loss : 0.3606 Acc : 0.89922\n",
            "(TEST) Correct_classified :  9763  of 10000\n",
            "(TEST) Epoch:    0 Loss : 0.3174 Acc : 0.9763\n",
            "\n",
            " ************************************************************ \n",
            "\n",
            "(TRAIN) Epoch:    1 batch_number:  100 Loss : 0.3077 Acc : 0.97094\n",
            "(TRAIN) Epoch:    1 batch_number:  200 Loss : 0.2918 Acc : 0.97383\n",
            "(TRAIN) Epoch:    1 batch_number:  300 Loss : 0.2732 Acc : 0.97385\n",
            "(TRAIN) Epoch:    1 batch_number:  400 Loss : 0.2153 Acc : 0.97479\n",
            "(TEST) Correct_classified :  9839  of 10000\n",
            "(TEST) Epoch:    1 Loss : 0.2507 Acc : 0.9839\n",
            "\n",
            " ************************************************************ \n",
            "\n",
            "(TRAIN) Epoch:    2 batch_number:  100 Loss : 0.1439 Acc : 0.97695\n",
            "(TRAIN) Epoch:    2 batch_number:  200 Loss : 0.1366 Acc : 0.97922\n",
            "(TRAIN) Epoch:    2 batch_number:  300 Loss : 0.1248 Acc : 0.98029\n",
            "(TRAIN) Epoch:    2 batch_number:  400 Loss : 0.1632 Acc : 0.98074\n",
            "(TEST) Correct_classified :  9848  of 10000\n",
            "(TEST) Epoch:    2 Loss : 0.2098 Acc : 0.9848\n",
            "\n",
            " ************************************************************ \n",
            "\n",
            "(TRAIN) Epoch:    3 batch_number:  100 Loss : 0.1099 Acc : 0.98188\n",
            "(TRAIN) Epoch:    3 batch_number:  200 Loss : 0.1289 Acc : 0.98285\n",
            "(TRAIN) Epoch:    3 batch_number:  300 Loss : 0.08267 Acc : 0.98284\n",
            "(TRAIN) Epoch:    3 batch_number:  400 Loss : 0.07734 Acc : 0.98328\n",
            "(TEST) Correct_classified :  9857  of 10000\n",
            "(TEST) Epoch:    3 Loss : 0.1824 Acc : 0.9857\n",
            "\n",
            " ************************************************************ \n",
            "\n",
            "(TRAIN) Epoch:    4 batch_number:  100 Loss : 0.1119 Acc : 0.985\n",
            "(TRAIN) Epoch:    4 batch_number:  200 Loss : 0.09299 Acc : 0.98492\n",
            "(TRAIN) Epoch:    4 batch_number:  300 Loss : 0.06161 Acc : 0.98474\n",
            "(TRAIN) Epoch:    4 batch_number:  400 Loss : 0.07889 Acc : 0.98521\n",
            "(TEST) Correct_classified :  9872  of 10000\n",
            "(TEST) Epoch:    4 Loss : 0.1629 Acc : 0.9872\n",
            "\n",
            " ************************************************************ \n",
            "\n",
            "(TRAIN) Epoch:    5 batch_number:  100 Loss : 0.07761 Acc : 0.98672\n",
            "(TRAIN) Epoch:    5 batch_number:  200 Loss : 0.1036 Acc : 0.98707\n",
            "(TRAIN) Epoch:    5 batch_number:  300 Loss : 0.1137 Acc : 0.98701\n",
            "(TRAIN) Epoch:    5 batch_number:  400 Loss : 0.1064 Acc : 0.98666\n",
            "(TEST) Correct_classified :  9893  of 10000\n",
            "(TEST) Epoch:    5 Loss : 0.147 Acc : 0.9893\n",
            "\n",
            " ************************************************************ \n",
            "\n",
            "(TRAIN) Epoch:    6 batch_number:  100 Loss : 0.03336 Acc : 0.98758\n",
            "(TRAIN) Epoch:    6 batch_number:  200 Loss : 0.0574 Acc : 0.98773\n",
            "(TRAIN) Epoch:    6 batch_number:  300 Loss : 0.06179 Acc : 0.98781\n",
            "(TRAIN) Epoch:    6 batch_number:  400 Loss : 0.09329 Acc : 0.98797\n",
            "(TEST) Correct_classified :  9892  of 10000\n",
            "(TEST) Epoch:    6 Loss : 0.1349 Acc : 0.9892\n",
            "\n",
            " ************************************************************ \n",
            "\n",
            "(TRAIN) Epoch:    7 batch_number:  100 Loss : 0.03887 Acc : 0.98836\n",
            "(TRAIN) Epoch:    7 batch_number:  200 Loss : 0.05577 Acc : 0.9877\n",
            "(TRAIN) Epoch:    7 batch_number:  300 Loss : 0.05698 Acc : 0.9882\n",
            "(TRAIN) Epoch:    7 batch_number:  400 Loss : 0.06434 Acc : 0.98799\n",
            "(TEST) Correct_classified :  9897  of 10000\n",
            "(TEST) Epoch:    7 Loss : 0.1244 Acc : 0.9897\n",
            "\n",
            " ************************************************************ \n",
            "\n",
            "(TRAIN) Epoch:    8 batch_number:  100 Loss : 0.02165 Acc : 0.99047\n",
            "(TRAIN) Epoch:    8 batch_number:  200 Loss : 0.04473 Acc : 0.98988\n",
            "(TRAIN) Epoch:    8 batch_number:  300 Loss : 0.04117 Acc : 0.98984\n",
            "(TRAIN) Epoch:    8 batch_number:  400 Loss : 0.08317 Acc : 0.9899\n",
            "(TEST) Correct_classified :  9890  of 10000\n",
            "(TEST) Epoch:    8 Loss : 0.1168 Acc : 0.989\n",
            "\n",
            " ************************************************************ \n",
            "\n",
            "(TRAIN) Epoch:    9 batch_number:  100 Loss : 0.08766 Acc : 0.99086\n",
            "(TRAIN) Epoch:    9 batch_number:  200 Loss : 0.01742 Acc : 0.99082\n",
            "(TRAIN) Epoch:    9 batch_number:  300 Loss : 0.04565 Acc : 0.99073\n",
            "(TRAIN) Epoch:    9 batch_number:  400 Loss : 0.04763 Acc : 0.99049\n",
            "(TEST) Correct_classified :  9911  of 10000\n",
            "(TEST) Epoch:    9 Loss : 0.11 Acc : 0.9911\n",
            "\n",
            " ************************************************************ \n",
            "\n",
            "(TRAIN) Epoch:   10 batch_number:  100 Loss : 0.02925 Acc : 0.99187\n",
            "(TRAIN) Epoch:   10 batch_number:  200 Loss : 0.05434 Acc : 0.99105\n",
            "(TRAIN) Epoch:   10 batch_number:  300 Loss : 0.03117 Acc : 0.99062\n",
            "(TRAIN) Epoch:   10 batch_number:  400 Loss : 0.02756 Acc : 0.9907\n",
            "(TEST) Correct_classified :  9894  of 10000\n",
            "(TEST) Epoch:   10 Loss : 0.1042 Acc : 0.9894\n",
            "\n",
            " ************************************************************ \n",
            "\n",
            "(TRAIN) Epoch:   11 batch_number:  100 Loss : 0.0227 Acc : 0.99047\n",
            "(TRAIN) Epoch:   11 batch_number:  200 Loss : 0.03889 Acc : 0.99113\n",
            "(TRAIN) Epoch:   11 batch_number:  300 Loss : 0.04069 Acc : 0.99068\n",
            "(TRAIN) Epoch:   11 batch_number:  400 Loss : 0.02781 Acc : 0.99053\n",
            "(TEST) Correct_classified :  9886  of 10000\n",
            "(TEST) Epoch:   11 Loss : 0.09914 Acc : 0.9886\n",
            "\n",
            " ************************************************************ \n",
            "\n",
            "(TRAIN) Epoch:   12 batch_number:  100 Loss : 0.01826 Acc : 0.99086\n",
            "(TRAIN) Epoch:   12 batch_number:  200 Loss : 0.01718 Acc : 0.99148\n",
            "(TRAIN) Epoch:   12 batch_number:  300 Loss : 0.05193 Acc : 0.99154\n",
            "(TRAIN) Epoch:   12 batch_number:  400 Loss : 0.1167 Acc : 0.99109\n",
            "(TEST) Correct_classified :  9899  of 10000\n",
            "(TEST) Epoch:   12 Loss : 0.09446 Acc : 0.9899\n",
            "\n",
            " ************************************************************ \n",
            "\n",
            "(TRAIN) Epoch:   13 batch_number:  100 Loss : 0.01013 Acc : 0.99313\n",
            "(TRAIN) Epoch:   13 batch_number:  200 Loss : 0.04555 Acc : 0.99305\n",
            "(TRAIN) Epoch:   13 batch_number:  300 Loss : 0.06023 Acc : 0.99242\n",
            "(TRAIN) Epoch:   13 batch_number:  400 Loss : 0.06591 Acc : 0.99195\n",
            "(TEST) Correct_classified :  9901  of 10000\n",
            "(TEST) Epoch:   13 Loss : 0.09054 Acc : 0.9901\n",
            "\n",
            " ************************************************************ \n",
            "\n",
            "(TRAIN) Epoch:   14 batch_number:  100 Loss : 0.03903 Acc : 0.99234\n",
            "(TRAIN) Epoch:   14 batch_number:  200 Loss : 0.01137 Acc : 0.99242\n",
            "(TRAIN) Epoch:   14 batch_number:  300 Loss : 0.02274 Acc : 0.99219\n",
            "(TRAIN) Epoch:   14 batch_number:  400 Loss : 0.0262 Acc : 0.99199\n",
            "(TEST) Correct_classified :  9901  of 10000\n",
            "(TEST) Epoch:   14 Loss : 0.08696 Acc : 0.9901\n",
            "\n",
            " ************************************************************ \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7sSJ0Zzepdyy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RiJM2rYA8Z2v"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0pvsgcnbjPL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TaSk9Bw7OLZO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}