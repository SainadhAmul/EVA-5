## Annotation

https://www.robots.ox.ac.uk/~vgg/software/via/



## Model training RESNET 18
Researchers observed that it makes sense to affirm that “the deeper the better” when it comes to convolutional neural networks. This makes sense, since the models should be more capable (their flexibility to adapt to any space increase because they have a bigger parameter space to explore). However, it has been noticed that after some depth, the performance degrades.
This was one of the bottlenecks of VGG. They couldn’t go as deep as wanted, because they started to lose generalization capability


![image](https://user-images.githubusercontent.com/45446030/114864664-e4c86080-9e0e-11eb-9325-12b8082f6e27.png)


![image](https://user-images.githubusercontent.com/45446030/114864767-0295c580-9e0f-11eb-88f4-093aa7dadd44.png)



This model is trained on tiny image net 200 dataset
